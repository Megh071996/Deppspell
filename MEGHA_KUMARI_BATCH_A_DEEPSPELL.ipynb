{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MEGHA_KUMARI_BATCH_A_DEEPSPELL.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "vuWKGw_j4Y41",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **DeepSpell**\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### Now a days we are using lots of digital documentation and formal emails. And we are creating it manually which can cause spelling mistakes. Misspelling can be hindrance in understanding of documents. \n",
        "### If we will talk about one word, there can be four possibility for errors :\n",
        "\n",
        "### *   when you write extra letter in a word\n",
        "### *   when you transpose by mistake\n",
        "### *   when you type wrong letter \n",
        "### *   when you forget to write a letter\n",
        "\n",
        "\n",
        "### And this can be corrected in two steps:\n",
        "\n",
        "\n",
        "\n",
        "### *   Using cartesian product by creating probability of known words(dictionary words) and replace the misspelled word.\n",
        "### *   From the set of words, identifying the best possible words that is correct.\n",
        "\n",
        "\n",
        "## About Code :\n",
        "\n",
        "### Here they are using an algorithm for Spelling correction in English texts using machine learning. They are using artificial data set by manually adding errors in correct sentences. They are using data set of 286MB text file news that is a part of one billion words and train with character by character and sequence-to-sequence model with  Long Short Term Memory networks(a special kind of RNN) layers. Which is helping in conversion of sentences with error to correct text. \n",
        "### For cleaning the text we are using preprocessing pipeline, parses into lines, putting input as artificial spelling errors. And removed the space,dash,parenthesis etc.\n"
      ]
    },
    {
      "metadata": {
        "id": "CzeyvSoW4ibv",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    \"\"\"Clean the text - remove unwanted chars, fold punctuation etc.\"\"\"\n",
        "    result = NORMALIZE_WHITESPACE_REGEX.sub(' ', text.strip())\n",
        "    result = RE_DASH_FILTER.sub('-', result)\n",
        "    result = RE_APOSTROPHE_FILTER.sub(\"'\", result)\n",
        "    result = RE_LEFT_PARENTH_FILTER.sub(\"(\", result)\n",
        "    result = RE_RIGHT_PARENTH_FILTER.sub(\")\", result)\n",
        "    result = RE_BASIC_CLEANER.sub('', result)\n",
        "    return result\n",
        "\n",
        "def preprocesses_data_clean():\n",
        "    \"\"\"Pre-process the data - step 1 - cleanup\"\"\"\n",
        "    with open(NEWS_FILE_NAME_CLEAN, \"wb\") as clean_data:\n",
        "        for line in open(NEWS_FILE_NAME):\n",
        "            decoded_line = line.decode('utf-8')\n",
        "            cleaned_line = clean_text(decoded_line)\n",
        "            encoded_line = cleaned_line.encode(\"utf-8\")\n",
        "            clean_data.write(encoded_line + b\"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fbl2afIQ5DpC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Then breaking every lines into chunks. And each chunks is smaller than 40 words. Resulting pipeline is a set of question/answer pairs where question/answer are strings. Here question contain spelling mistakes and answer will be the string with approximatly correct strings with deep learning."
      ]
    },
    {
      "metadata": {
        "id": "pgBrhFi35Le2",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def preprocesses_split_lines():\n",
        "    LOGGER.info(\"Reading filtered data:\")\n",
        "    answers = set()\n",
        "    with open(NEWS_FILE_NAME_SPLIT, \"wb\") as output_file:\n",
        "        for _line in open(NEWS_FILE_NAME_FILTERED):\n",
        "            line = _line.decode('utf-8')\n",
        "            while len(line) > MIN_INPUT_LEN:\n",
        "                if len(line) <= CONFIG.max_input_len:\n",
        "                    answer = line\n",
        "                    line = \"\"\n",
        "                else:\n",
        "                    space_location = line.rfind(\" \", MIN_INPUT_LEN, CONFIG.max_input_len - 1)\n",
        "                    if space_location > -1:\n",
        "                        answer = line[:space_location]\n",
        "                        line = line[len(answer) + 1:]\n",
        "                    else:\n",
        "                        space_location = line.rfind(\" \") # no limits this time\n",
        "                        if space_location == -1:\n",
        "                            break # we are done with this line\n",
        "                        else:\n",
        "                            line = line[space_location + 1:]\n",
        "                            continue\n",
        "                answers.add(answer)\n",
        "                output_file.write(answer.encode('utf-8') + b\"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wY-fRKMC5Q_u",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###They are using 90% of data as training dataset and 10% is learning dataset. Here they are using small preprocessed  and kept most of the data most popular characters. \n",
        "###They are using noise data to process by adding artificial error dataset."
      ]
    },
    {
      "metadata": {
        "id": "7tPVYwkL5kF5",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def add_noise_to_string(a_string, amount_of_noise):\n",
        "    \"\"\"Add some artificial spelling mistakes to the string\"\"\"\n",
        "    if rand() < amount_of_noise * len(a_string):\n",
        "        # Replace a character with a random character\n",
        "        random_char_position = random_randint(len(a_string))\n",
        "        a_string = a_string[:random_char_position] + random_choice(CHARS[:-1]) + a_string[random_char_position + 1:]\n",
        "    if rand() < amount_of_noise * len(a_string):\n",
        "        # Delete a character\n",
        "        random_char_position = random_randint(len(a_string))\n",
        "        a_string = a_string[:random_char_position] + a_string[random_char_position + 1:]\n",
        "    if len(a_string) < CONFIG.max_input_len and rand() < amount_of_noise * len(a_string):\n",
        "        # Add a random character\n",
        "        random_char_position = random_randint(len(a_string))\n",
        "        a_string = a_string[:random_char_position] + random_choice(CHARS[:-1]) + a_string[random_char_position:]\n",
        "    if rand() < amount_of_noise * len(a_string):\n",
        "        # Transpose 2 characters\n",
        "        random_char_position = random_randint(len(a_string) - 1)\n",
        "        a_string = (a_string[:random_char_position] + a_string[random_char_position + 1] + a_string[random_char_position] +\n",
        "                    a_string[random_char_position + 2:])\n",
        "    return a_string"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2aNaitxr59cE",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def generate_question(answer):\n",
        "    \"\"\"Generate a question by adding noise\"\"\"\n",
        "    question = add_noise_to_string(answer, AMOUNT_OF_NOISE)\n",
        "    # Add padding:\n",
        "    question += PADDING * (CONFIG.max_input_len - len(question))\n",
        "    answer += PADDING * (CONFIG.max_input_len - len(answer))\n",
        "    return question, answer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "auu9ejiJ5_Yd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###They are using vector based approach. The model learns the association of other words with each other and the pattern in which it occures. From this steps, each word would be converted to a vector and same related words will have vectors which are closer in distance. "
      ]
    },
    {
      "metadata": {
        "id": "p3RoViZH6Ewl",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def _vectorize(questions, answers, ctable):\n",
        "    \"\"\"Vectorize the data as numpy arrays\"\"\"\n",
        "    len_of_questions = len(questions)\n",
        "    X = np_zeros((len_of_questions, CONFIG.max_input_len, ctable.size), dtype=np.bool)\n",
        "    for i in xrange(len(questions)):\n",
        "        sentence = questions.pop()\n",
        "        for j, c in enumerate(sentence):\n",
        "            try:\n",
        "                X[i, j, ctable.char_indices[c]] = 1\n",
        "            except KeyError:\n",
        "                pass # Padding\n",
        "    y = np_zeros((len_of_questions, CONFIG.max_input_len, ctable.size), dtype=np.bool)\n",
        "    for i in xrange(len(answers)):\n",
        "        sentence = answers.pop()\n",
        "        for j, c in enumerate(sentence):\n",
        "            try:\n",
        "                y[i, j, ctable.char_indices[c]] = 1\n",
        "            except KeyError:\n",
        "                pass # Padding\n",
        "    return X, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0JHZ2fJ76MF3",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def vectorize(questions, answers, chars=None):\n",
        "    \"\"\"Vectorize the questions and expected answers\"\"\"\n",
        "    print('Vectorization...')\n",
        "    chars = chars or CHARS\n",
        "    ctable = CharacterTable(chars)\n",
        "    X, y = _vectorize(questions, answers, ctable)\n",
        "    # Explicitly set apart 10% for validation data that we never train over\n",
        "    split_at = int(len(X) - len(X) / 10)\n",
        "    (X_train, X_val) = (slice_X(X, 0, split_at), slice_X(X, split_at))\n",
        "    (y_train, y_val) = (y[:split_at], y[split_at:])\n",
        "\n",
        "    print(X_train.shape)\n",
        "    print(y_train.shape)\n",
        "\n",
        "    return X_train, X_val, y_train, y_val, CONFIG.max_input_len, ctable"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "X0xtsTn86Plu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### When we create the set, misspelled word will replace by corrected word and pass the set to word2vector deep learning engine. Then they identify the vector distance between context words and word that have to replace, wchich is closest to  the vector of nearby words. And it is selected as the final output.\n",
        "\n",
        "## Future Improvement :\n",
        "\n",
        "### *   Dataset are artificial, we can use real life typing mistake dataset\n",
        "### *   We can use current version of keras\n",
        "### *   We can use TenserFlow API\n",
        "### *   We can use more larger dataset.\n",
        "\n",
        "---\n",
        "#Thank You :)\n",
        "---"
      ]
    }
  ]
}